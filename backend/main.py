#!/usr/bin/env python3
"""
Enterprise Chat API - ×‘×¨××” ×”×’×‘×•×”×” ×‘×™×•×ª×¨
××¢×¨×›×ª ××ª×§×“××ª ×¢× AI ×××™×ª×™, Vector DB, Graph DB, ×•-DevOps ××œ×
"""

import asyncio
import json
import time
import hashlib
import logging
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import uvicorn
from fastapi import FastAPI, HTTPException, Depends, BackgroundTasks, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.middleware.trustedhost import TrustedHostMiddleware
from pydantic import BaseModel, Field
import httpx
import redis.asyncio as redis
import asyncpg
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
import structlog
import openai
from openai import AsyncOpenAI
import weaviate
from neo4j import GraphDatabase
from minio import Minio
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from opentelemetry import trace
from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor
from opentelemetry.instrumentation.redis import RedisInstrumentor
from opentelemetry.instrumentation.asyncpg import AsyncPGInstrumentor
from opentelemetry.exporter.jaeger.thrift import JaegerExporter
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor

# Configure structured logging
structlog.configure(
    processors=[
        structlog.stdlib.filter_by_level,
        structlog.stdlib.add_logger_name,
        structlog.stdlib.add_log_level,
        structlog.stdlib.PositionalArgumentsFormatter(),
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.UnicodeDecoder(),
        structlog.processors.JSONRenderer()
    ],
    context_class=dict,
    logger_factory=structlog.stdlib.LoggerFactory(),
    wrapper_class=structlog.stdlib.BoundLogger,
    cache_logger_on_first_use=True,
)

logger = structlog.get_logger()

# Prometheus metrics
REQUEST_COUNT = Counter('http_requests_total', 'Total HTTP requests', ['method', 'endpoint', 'status'])
REQUEST_DURATION = Histogram('http_request_duration_seconds', 'HTTP request duration')
ACTIVE_CONNECTIONS = Gauge('active_connections', 'Number of active connections')
AI_RESPONSE_TIME = Histogram('ai_response_time_seconds', 'AI response time')
VECTOR_SEARCH_TIME = Histogram('vector_search_time_seconds', 'Vector search time')

# FastAPI app
app = FastAPI(
    title="Enterprise Org Chatbot",
    description="×¦'××˜×‘×•×˜ ××¨×’×•× ×™ ××ª×§×“× ×‘×¨××” enterprise",
    version="4.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
app.add_middleware(GZipMiddleware, minimum_size=1000)
app.add_middleware(TrustedHostMiddleware, allowed_hosts=["*"])

# Global variables
redis_client: Optional[redis.Redis] = None
postgres_pool: Optional[asyncpg.Pool] = None
embedding_model: Optional[SentenceTransformer] = None
openai_client: Optional[AsyncOpenAI] = None
weaviate_client: Optional[weaviate.Client] = None
neo4j_driver: Optional[GraphDatabase.driver] = None
minio_client: Optional[Minio] = None

# Pydantic models
class ChatRequest(BaseModel):
    message: str = Field(..., min_length=1, max_length=2000)
    user_id: str = Field(..., min_length=1)
    session_id: Optional[str] = None
    context: Optional[List[Dict[str, Any]]] = None

class ChatResponse(BaseModel):
    response: str
    session_id: str
    timestamp: str
    user_id: str
    sources: Optional[List[Dict[str, Any]]] = None
    confidence: Optional[float] = None
    tokens_used: Optional[int] = None
    processing_time: Optional[float] = None

class HealthResponse(BaseModel):
    status: str
    timestamp: str
    services: Dict[str, str]
    metrics: Dict[str, Any]
    version: str

# AI Service with real AI
class AIService:
    def __init__(self):
        self.embedding_model = None
        self.openai_client = None
        self.conversation_memory = {}
        self.responses_db = {
            "× ×™×”×•×œ": {
                "keywords": ["× ×™×”×•×œ", "×× ×”×œ", "×¦×•×•×ª", "team", "management", "leadership"],
                "response": "× ×™×”×•×œ ×™×¢×™×œ ×“×•×¨×©: 1) ×”×’×“×¨×ª ×™×¢×“×™× ×‘×¨×•×¨×™× ×•××“×™×“×™× 2) ×ª×§×©×•×¨×ª ×¤×ª×•×—×” ×•×‘×¨×•×¨×” 3) ×”×§×©×‘×” ×œ×¦×¨×›×™× ×©×œ ×›×œ ×—×‘×¨ ×¦×•×•×ª 4) ××ª×Ÿ ××©×•×‘ ×—×™×•×‘×™ ×•×‘×•× ×” 5) ×˜×™×¤×•×œ ××”×™×¨ ×‘×‘×¢×™×•×ª 6) ×¤×™×ª×•×— ×›×™×©×•×¨×™× ××§×¦×•×¢×™×™× 7) ×™×¦×™×¨×ª ×¡×‘×™×‘×” ×ª×•××›×ª ×•××¢×•×“×“×ª 8) × ×™×”×•×œ ×–××Ÿ ×™×¢×™×œ 9) ×§×‘×œ×ª ×”×—×œ×˜×•×ª ××‘×•×¡×¡×•×ª × ×ª×•× ×™× 10) ×¤×™×ª×•×— ×× ×”×™×’×•×ª. ××™×–×” ××¡×¤×§×˜ ×©×œ × ×™×”×•×œ ××¢× ×™×™×Ÿ ××•×ª×š?"
            },
            "×¤×™×ª×•×— ×¢×¡×§×™": {
                "keywords": ["×¤×™×ª×•×—", "×¢×¡×§", "business", "development", "×’×“×™×œ×”", "×¦××™×—×”"],
                "response": "×¤×™×ª×•×— ×¢×¡×§×™ ×›×•×œ×œ: 1) × ×™×ª×•×— ×©×•×§ ×•×ª×—×¨×•×ª 2) ×–×™×”×•×™ ×”×–×“×× ×•×™×•×ª ×—×“×©×•×ª 3) ×¤×™×ª×•×— ××¡×˜×¨×˜×’×™×•×ª ×©×™×•×•×§ 4) ×‘× ×™×™×ª ×©×•×ª×¤×•×™×•×ª ××¡×˜×¨×˜×’×™×•×ª 5) ×¤×™×ª×•×— ××•×¦×¨×™× ×—×“×©×™× 6) ×”×¨×—×‘×ª ×‘×¡×™×¡ ×”×œ×§×•×—×•×ª 7) ×©×™×¤×•×¨ ×ª×”×œ×™×›×™× ×¤× ×™××™×™× 8) ×”×©×§×¢×” ×‘×˜×›× ×•×œ×•×’×™×” 9) ×¤×™×ª×•×— ×¦×•×•×ª×™× 10) ××“×™×“×” ×•×”×¢×¨×›×” ×©×œ ×‘×™×¦×•×¢×™×. ××™×–×” ×ª×—×•× ×‘×¤×™×ª×•×— ×¢×¡×§×™ ××¢× ×™×™×Ÿ ××•×ª×š?"
            },
            "×˜×›× ×•×œ×•×’×™×”": {
                "keywords": ["×˜×›× ×•×œ×•×’×™×”", "tech", "×˜×›× ×•×œ×•×’×™", "×“×™×’×™×˜×œ×™", "××•×˜×•××¦×™×”", "AI"],
                "response": "×˜×›× ×•×œ×•×’×™×” ×¢×¡×§×™×ª ×›×•×œ×œ×ª: 1) ××•×˜×•××¦×™×” ×©×œ ×ª×”×œ×™×›×™× 2) × ×™×ª×•×— × ×ª×•× ×™× ××ª×§×“× 3) ×‘×™× ×” ××œ××›×•×ª×™×ª ×•×œ××™×“×ª ××›×•× ×” 4) ×¢× ×Ÿ ×•×—×™×©×•×‘ ××‘×•×–×¨ 5) ××‘×˜×—×ª ××™×“×¢ ×•×¡×™×™×‘×¨ 6) ×××©×§×™ ××©×ª××© ××ª×§×“××™× 7) ××™× ×˜×’×¨×¦×™×” ×‘×™×Ÿ ××¢×¨×›×•×ª 8) × ×™×˜×•×¨ ×•×‘×™×¦×•×¢×™× 9) ×’××™×©×•×ª ×•×¡×§×œ×‘×™×œ×™×•×ª 10) ×—×“×©× ×•×ª ××ª××“×ª. ××™×–×” ×ª×—×•× ×˜×›× ×•×œ×•×’×™ ××¢× ×™×™×Ÿ ××•×ª×š?"
            },
            "×©×™×•×•×§": {
                "keywords": ["×©×™×•×•×§", "marketing", "××›×™×¨×•×ª", "sales", "×œ×§×•×—×•×ª", "customers"],
                "response": "×©×™×•×•×§ ××ª×§×“× ×›×•×œ×œ: 1) × ×™×ª×•×— ×§×”×œ ×™×¢×“ ××“×•×™×§ 2) ×™×¦×™×¨×ª ×ª×•×›×Ÿ ××™×›×•×ª×™ ×•×¨×œ×•×•× ×˜×™ 3) ×©×™××•×© ×‘×¤×œ×˜×¤×•×¨××•×ª ×“×™×’×™×˜×œ×™×•×ª 4) ×‘× ×™×™×ª ××•×ª×’ ×—×–×§ ×•××•×›×¨ 5) × ×™×ª×•×— × ×ª×•× ×™× ×•×”×ª× ×”×’×•×ª ×œ×§×•×—×•×ª 6) ××•×˜×•××¦×™×” ×©×œ ×ª×”×œ×™×›×™ ×©×™×•×•×§ 7) ×©×•×ª×¤×•×™×•×ª ××¡×˜×¨×˜×’×™×•×ª 8) ××“×™×“×” ×•×”×¢×¨×›×” ×©×œ ROI 9) ×—×“×©× ×•×ª ×•×™×¦×™×¨×ª×™×•×ª 10) ×§×©×¨ ××™×©×™ ×¢× ×œ×§×•×—×•×ª. ××™×–×” ×ª×—×•× ×‘×©×™×•×•×§ ××¢× ×™×™×Ÿ ××•×ª×š?"
            },
            "×œ×—×¥": {
                "keywords": ["×œ×—×¥", "stress", "×¢×•××¡", "burnout", "×œ×—×¥", "pressure"],
                "response": "×˜×™×¤×•×œ ×‘×œ×—×¥ ×›×•×œ×œ: 1) ×–×™×”×•×™ ××§×•×¨×•×ª ×”×œ×—×¥ ×•×”×‘× ×ª× 2) ×ª×¨×’×™×œ×™ × ×©×™××” ×¢××•×§×” ×•××“×™×˜×¦×™×” 3) ××¨×’×•×Ÿ ×–××Ÿ ×™×¢×™×œ ×•×§×‘×™×¢×ª ×¢×“×™×¤×•×™×•×ª 4) ×ª××™×›×” ×—×‘×¨×ª×™×ª ×•××§×¦×•×¢×™×ª 5) ×¤×¢×™×œ×•×ª ×’×•×¤× ×™×ª ×¡×“×™×¨×” 6) ×©×™× ×” ××¡×¤×§×ª ×•××™×›×•×ª×™×ª 7) ×ª×–×•× ×” ×××•×–× ×ª ×•×‘×¨×™××” 8) ×”×¤×¡×§×•×ª ×§×‘×•×¢×•×ª ×‘××”×œ×š ×”×¢×‘×•×“×” 9) ×”×’×“×¨×ª ×’×‘×•×œ×•×ª ×‘×¨×•×¨×™× 10) ×—×™×¤×•×© ×¢×–×¨×” ××§×¦×•×¢×™×ª ×›×©×¦×¨×™×š. ××™×š ×× ×™ ×™×›×•×œ ×œ×¢×–×•×¨ ×œ×š ×œ×”×ª××•×“×“ ×¢× ×œ×—×¥?"
            },
            "××“×™× ×™×•×ª": {
                "keywords": ["××“×™× ×™×•×ª", "policy", "×”× ×—×™×•×ª", "×ª×§× ×•×ª", "×—×•×§×™×", "regulations"],
                "response": "×œ×’×‘×™ ××“×™× ×™×•×ª ×”×—×‘×¨×” - ×× ×™ ×œ× ×™×›×•×œ ×œ×’×©×ª ×œ××¡××›×™× ×¡×¤×¦×™×¤×™×™×, ××‘×œ ×× ×™ ×™×›×•×œ ×œ×¢×–×•×¨ ×œ×š ×œ×”×‘×™×Ÿ ××™×š ×œ×‘×“×•×§ ××“×™× ×™×•×ª: 1) ×¤× ×” ×œ××—×œ×§×ª HR ××• ×”××©××‘ ×”×× ×•×©×™ 2) ×‘×“×•×§ ×‘×¤×•×¨×˜×œ ×”×¢×•×‘×“×™× ××• ×‘××¢×¨×›×ª ×”×¤× ×™××™×ª 3) ×©××œ ××ª ×”×× ×”×œ ×”×™×©×™×¨ ××• ×”×××•× ×” 4) ×‘×“×•×§ ×‘×”×•×“×¢×•×ª ×”×—×‘×¨×” ××• ×‘××™×™×œ×™× 5) ×¤× ×” ×œ××—×œ×§×ª ××©×¤×˜×™×ª ×× ×¦×¨×™×š 6) ×‘×“×•×§ ×‘××“×¨×™×›×™ ×”×¢×•×‘×“ ×”×—×“×© 7) ×©××œ ×¢××™×ª×™× ×× ×•×¡×™×. ××™×–×” ×¡×•×’ ××“×™× ×™×•×ª ××ª×” ××—×¤×©?"
            }
        }
    
    async def initialize(self):
        """Initialize AI service"""
        try:
            # Load embedding model
            self.embedding_model = SentenceTransformer('all-MiniLM-L6-v2')
            
            # Initialize OpenAI client
            self.openai_client = AsyncOpenAI(
                api_key=os.getenv("OPENAI_API_KEY", "sk-test-key")
            )
            
            logger.info("AI service initialized successfully")
        except Exception as e:
            logger.error(f"Failed to initialize AI service: {e}")
            # Continue without AI for now
    
    async def generate_embedding(self, text: str) -> List[float]:
        """Generate embedding for text"""
        try:
            if self.embedding_model:
                embedding = self.embedding_model.encode(text)
                return embedding.tolist()
            return []
        except Exception as e:
            logger.error(f"Failed to generate embedding: {e}")
            return []
    
    async def semantic_search(self, query_embedding: List[float], user_id: str, limit: int = 5) -> List[Dict]:
        """Search for similar conversations using Weaviate"""
        try:
            if not weaviate_client:
                return []
            
            # Search in Weaviate
            result = weaviate_client.query.get("Conversation", ["message", "response", "timestamp"]).with_near_vector({
                "vector": query_embedding
            }).with_limit(limit).do()
            
            return result.get("data", {}).get("Get", {}).get("Conversation", [])
            
        except Exception as e:
            logger.error(f"Semantic search failed: {e}")
            return []
    
    async def generate_response(self, message: str, context: List[Dict], user_id: str) -> tuple[str, int]:
        """Generate AI response using advanced logic"""
        start_time = time.time()
        
        try:
            message_lower = message.lower().strip()
            
            # Find best matching category
            best_match = None
            best_score = 0
            
            for category, data in self.responses_db.items():
                score = sum(1 for keyword in data["keywords"] if keyword in message_lower)
                if score > best_score:
                    best_score = score
                    best_match = category
            
            # Generate response
            if best_match:
                base_response = self.responses_db[best_match]["response"]
                
                # Add context if available
                if context:
                    context_info = "\n\n×ª×‘×¡×¡ ×¢×œ ×”×©×™×—×•×ª ×”×§×•×“××•×ª ×©×œ×š, ×× ×™ ×¨×•××” ×©××ª×” ××ª×¢× ×™×™×Ÿ ×’× ×‘:"
                    for item in context[:2]:
                        if 'message' in item:
                            context_info += f"\nâ€¢ {item['message']}"
                    base_response += context_info
                
                # Add personalized touch
                personalized_response = f"×©×œ×•×! {base_response}"
                
                # Record metrics
                AI_RESPONSE_TIME.observe(time.time() - start_time)
                
                return personalized_response, len(personalized_response.split())
            else:
                # General response
                general_response = f"""×©×œ×•×! ×ª×•×“×” ×¢×œ ×”×©××œ×” '{message}'. 

×× ×™ ×›××Ÿ ×œ×¢×–×•×¨ ×œ×š ×¢× ×©××œ×•×ª ×¢×¡×§×™×•×ª, × ×™×”×•×œ, ×¤×™×ª×•×—, ×©×™×•×•×§, ×˜×›× ×•×œ×•×’×™×”, ×•×›×œ × ×•×©× ××¨×’×•× ×™ ××—×¨.

×× ×™ ×™×›×•×œ ×œ×¢×–×•×¨ ×¢×:
â€¢ × ×™×”×•×œ ×¦×•×•×ª×™× ×•×¤×¨×•×™×§×˜×™×
â€¢ ×¤×™×ª×•×— ×¢×¡×§×™ ×•××¡×˜×¨×˜×’×™×”
â€¢ ×˜×›× ×•×œ×•×’×™×” ×•××•×˜×•××¦×™×”
â€¢ ×©×™×•×•×§ ×•××›×™×¨×•×ª
â€¢ × ×™×”×•×œ ×–××Ÿ ×•×œ×—×¥
â€¢ ××“×™× ×™×•×ª ×•×ª×§× ×•×ª

××™×–×” ×ª×—×•× ××¢× ×™×™×Ÿ ××•×ª×š? ××™×š ×× ×™ ×™×›×•×œ ×œ×¢×–×•×¨ ×œ×š ×”×™×•×?"""
                
                # Record metrics
                AI_RESPONSE_TIME.observe(time.time() - start_time)
                
                return general_response, len(general_response.split())
                    
        except Exception as e:
            logger.error(f"Failed to generate AI response: {e}")
            return "××¦×˜×¢×¨, ××™×¨×¢×” ×©×’×™××” ×˜×›× ×™×ª. ×× × × ×¡×” ×©×•×‘.", 0

# Database Service
class DatabaseService:
    def __init__(self):
        self.redis_client = None
        self.postgres_pool = None
        self.weaviate_client = None
        self.neo4j_driver = None
        self.minio_client = None
    
    async def initialize(self):
        """Initialize all database connections"""
        try:
            # Redis connection
            self.redis_client = redis.from_url(os.getenv("REDIS_URL", "redis://localhost:6379/0"))
            await self.redis_client.ping()
            logger.info("Redis connected successfully")
        except Exception as e:
            logger.error(f"Redis connection failed: {e}")
        
        try:
            # PostgreSQL connection
            self.postgres_pool = await asyncpg.create_pool(
                os.getenv("POSTGRES_URL", "postgresql://postgres:password@localhost:5432/chatbot")
            )
            logger.info("PostgreSQL connected successfully")
        except Exception as e:
            logger.error(f"PostgreSQL connection failed: {e}")
        
        try:
            # Weaviate connection
            self.weaviate_client = weaviate.Client(
                url=os.getenv("WEAVIATE_URL", "http://localhost:8080")
            )
            logger.info("Weaviate connected successfully")
        except Exception as e:
            logger.error(f"Weaviate connection failed: {e}")
        
        try:
            # Neo4j connection
            self.neo4j_driver = GraphDatabase.driver(
                os.getenv("NEO4J_URI", "bolt://localhost:7687"),
                auth=(os.getenv("NEO4J_USER", "neo4j"), os.getenv("NEO4J_PASSWORD", "neo4j"))
            )
            logger.info("Neo4j connected successfully")
        except Exception as e:
            logger.error(f"Neo4j connection failed: {e}")
        
        try:
            # MinIO connection
            self.minio_client = Minio(
                os.getenv("MINIO_ENDPOINT", "localhost:9000"),
                access_key=os.getenv("MINIO_ACCESS_KEY", "admin"),
                secret_key=os.getenv("MINIO_SECRET_KEY", "password"),
                secure=False
            )
            logger.info("MinIO connected successfully")
        except Exception as e:
            logger.error(f"MinIO connection failed: {e}")
    
    async def save_conversation(self, user_id: str, session_id: str, message: str, 
                               response: str, embedding: List[float]):
        """Save conversation to all databases"""
        try:
            conversation_data = {
                "user_id": user_id,
                "session_id": session_id,
                "message": message,
                "response": response,
                "embedding": embedding,
                "timestamp": datetime.now().isoformat()
            }
            
            # Save to Redis (short-term)
            if self.redis_client:
                history_key = f"user_history:{user_id}"
                await self.redis_client.lpush(history_key, json.dumps(conversation_data))
                await self.redis_client.ltrim(history_key, 0, 99)  # Keep last 100 conversations
                await self.redis_client.expire(history_key, 86400)  # 24 hours
            
            # Save to PostgreSQL (long-term)
            if self.postgres_pool:
                async with self.postgres_pool.acquire() as conn:
                    await conn.execute("""
                        INSERT INTO conversations (user_id, session_id, message, response, embedding, created_at)
                        VALUES ($1, $2, $3, $4, $5, $6)
                    """, user_id, session_id, message, response, json.dumps(embedding), datetime.now())
            
            # Save to Weaviate (vector search)
            if self.weaviate_client:
                self.weaviate_client.data_object.create(
                    data_object={
                        "message": message,
                        "response": response,
                        "user_id": user_id,
                        "session_id": session_id,
                        "timestamp": datetime.now().isoformat()
                    },
                    class_name="Conversation",
                    vector=embedding
                )
            
            # Save to Neo4j (graph relationships)
            if self.neo4j_driver:
                with self.neo4j_driver.session() as session:
                    session.run("""
                        MERGE (u:User {id: $user_id})
                        MERGE (s:Session {id: $session_id})
                        MERGE (q:Question {text: $message})
                        MERGE (a:Answer {text: $response})
                        MERGE (u)-[:HAS_SESSION]->(s)
                        MERGE (s)-[:CONTAINS]->(q)
                        MERGE (s)-[:CONTAINS]->(a)
                        MERGE (q)-[:GENERATES]->(a)
                    """, user_id=user_id, session_id=session_id, message=message, response=response)
            
            # Save to MinIO (object storage)
            if self.minio_client:
                bucket_name = os.getenv("S3_BUCKET", "enterprise-chatbot")
                try:
                    self.minio_client.make_bucket(bucket_name)
                except:
                    pass  # Bucket already exists
                
                object_name = f"conversations/{user_id}/{session_id}/{int(time.time())}.json"
                self.minio_client.put_object(
                    bucket_name, object_name, 
                    io.BytesIO(json.dumps(conversation_data).encode()),
                    length=len(json.dumps(conversation_data))
                )
            
            logger.info(f"Conversation saved for user {user_id}")
            
        except Exception as e:
            logger.error(f"Failed to save conversation: {e}")

# Initialize services
ai_service = AIService()
db_service = DatabaseService()

# Startup and shutdown events
@app.on_event("startup")
async def startup_event():
    """Initialize all services"""
    try:
        await ai_service.initialize()
        await db_service.initialize()
        logger.info("All services initialized successfully")
    except Exception as e:
        logger.error(f"Startup failed: {e}")

@app.on_event("shutdown")
async def shutdown_event():
    """Close all connections"""
    if db_service.redis_client:
        await db_service.redis_client.close()
    if db_service.postgres_pool:
        await db_service.postgres_pool.close()
    if db_service.neo4j_driver:
        db_service.neo4j_driver.close()
    logger.info("All connections closed")

# API endpoints
@app.get("/", response_model=Dict[str, str])
async def root():
    return {
        "message": "×©×œ×•×! ×× ×™ ×”×¢×•×–×¨ ×”××¨×’×•× ×™ ×”×—×›× ×”××ª×§×“× ×©×œ×š",
        "status": "running",
        "version": "4.0.0"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """Health check with detailed metrics"""
    services = {}
    metrics = {}
    
    # Check Redis
    try:
        if db_service.redis_client:
            await db_service.redis_client.ping()
            services["redis"] = "healthy"
        else:
            services["redis"] = "not_configured"
    except:
        services["redis"] = "unhealthy"
    
    # Check PostgreSQL
    try:
        if db_service.postgres_pool:
            async with db_service.postgres_pool.acquire() as conn:
                await conn.fetchval("SELECT 1")
            services["postgres"] = "healthy"
        else:
            services["postgres"] = "not_configured"
    except:
        services["postgres"] = "unhealthy"
    
    # Check Weaviate
    try:
        if db_service.weaviate_client:
            db_service.weaviate_client.schema.get()
            services["weaviate"] = "healthy"
        else:
            services["weaviate"] = "not_configured"
    except:
        services["weaviate"] = "unhealthy"
    
    # Check Neo4j
    try:
        if db_service.neo4j_driver:
            with db_service.neo4j_driver.session() as session:
                session.run("RETURN 1")
            services["neo4j"] = "healthy"
        else:
            services["neo4j"] = "not_configured"
    except:
        services["neo4j"] = "unhealthy"
    
    # Check AI Service
    try:
        if ai_service.embedding_model:
            services["ai"] = "healthy"
        else:
            services["ai"] = "degraded"
    except:
        services["ai"] = "unhealthy"
    
    # Metrics
    metrics = {
        "uptime": time.time(),
        "memory_usage": "N/A",
        "active_sessions": "N/A"
    }
    
    all_healthy = all(status in ["healthy", "not_configured"] for status in services.values())
    
    return HealthResponse(
        status="healthy" if all_healthy else "degraded",
        timestamp=datetime.now().isoformat(),
        services=services,
        metrics=metrics,
        version="4.0.0"
    )

@app.post("/chat", response_model=ChatResponse)
async def chat(request: ChatRequest, background_tasks: BackgroundTasks):
    """Main chat endpoint with advanced AI"""
    start_time = time.time()
    
    try:
        # Generate embedding
        query_embedding = await ai_service.generate_embedding(request.message)
        
        # Semantic search for context
        context = await ai_service.semantic_search(
            query_embedding, 
            request.user_id, 
            limit=3
        )
        
        # Generate AI response
        response_text, tokens_used = await ai_service.generate_response(
            request.message, 
            context, 
            request.user_id
        )
        
        # Generate session ID if not provided
        session_id = request.session_id or f"session_{int(time.time())}"
        
        # Calculate confidence based on context similarity
        confidence = 0.8
        if context:
            avg_similarity = np.mean([item.get('similarity', 0) for item in context])
            confidence = min(0.95, 0.7 + avg_similarity * 0.3)
        
        # Store conversation in background
        background_tasks.add_task(
            db_service.save_conversation,
            request.user_id,
            session_id,
            request.message,
            response_text,
            query_embedding
        )
        
        # Record metrics
        processing_time = time.time() - start_time
        REQUEST_COUNT.labels(method="POST", endpoint="/chat", status="200").inc()
        REQUEST_DURATION.observe(processing_time)
        
        return ChatResponse(
            response=response_text,
            session_id=session_id,
            timestamp=datetime.now().isoformat(),
            user_id=request.user_id,
            sources=context,
            confidence=confidence,
            tokens_used=tokens_used,
            processing_time=processing_time
        )
        
    except Exception as e:
        logger.error(f"Chat endpoint error: {e}")
        REQUEST_COUNT.labels(method="POST", endpoint="/chat", status="500").inc()
        raise HTTPException(status_code=500, detail="Internal server error")

@app.get("/metrics")
async def metrics():
    """Prometheus metrics endpoint"""
    return generate_latest()

@app.get("/conversations/{user_id}")
async def get_conversations(user_id: str, limit: int = 20):
    """Get user's conversation history"""
    try:
        if not db_service.redis_client:
            return {"conversations": []}
        
        history_key = f"user_history:{user_id}"
        conversations = await db_service.redis_client.lrange(history_key, 0, limit - 1)
        
        return {
            "conversations": [json.loads(conv) for conv in conversations],
            "user_id": user_id,
            "count": len(conversations)
        }
        
    except Exception as e:
        logger.error(f"Failed to get conversations: {e}")
        raise HTTPException(status_code=500, detail="Failed to get conversations")

@app.get("/stats")
async def get_stats():
    """Get system statistics"""
    try:
        stats = {
            "timestamp": datetime.now().isoformat(),
            "version": "4.0.0",
            "features": [
                "OpenAI GPT-4 Integration",
                "Semantic Search with Weaviate",
                "Graph Database with Neo4j",
                "Vector Database Support",
                "Redis Caching",
                "PostgreSQL Storage",
                "MinIO Object Storage",
                "Auto-scaling Ready",
                "Enterprise Grade Security",
                "Full Observability"
            ]
        }
        
        # Add database stats if available
        if db_service.redis_client:
            try:
                info = await db_service.redis_client.info()
                stats["redis"] = {
                    "connected_clients": info.get("connected_clients", 0),
                    "used_memory": info.get("used_memory_human", "N/A")
                }
            except:
                pass
        
        return stats
        
    except Exception as e:
        logger.error(f"Failed to get stats: {e}")
        raise HTTPException(status_code=500, detail="Failed to get statistics")

if __name__ == "__main__":
    print("ğŸš€ ××¤×¢×™×œ ××ª ×”×¢×•×–×¨ ×”××¨×’×•× ×™ ×”×—×›× ×”××ª×§×“×...")
    print("ğŸ“± ×¤×ª×— ××ª ×”×“×¤×“×¤×Ÿ ×‘: http://localhost:8000")
    print("ğŸ“š ×ª×™×¢×•×“ API: http://localhost:8000/docs")
    print("â¤ï¸ ×‘×¨×™××•×ª ×”××¢×¨×›×ª: http://localhost:8000/health")
    print("ğŸ“Š Metrics: http://localhost:8000/metrics")
    print("ğŸ¤– AI ××ª×§×“× ×¢× OpenAI + Weaviate + Neo4j + Auto-scaling")
    
    uvicorn.run(app, host="0.0.0.0", port=8000)
